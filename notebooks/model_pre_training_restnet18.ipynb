{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#pip install torch==2.1.0 torchvision==0.16.0 torchaudio==2.1.0 --index-url https://download.pytorch.org/whl/cu118\n",
    "#pip install torch==2.1.0 torchvision==0.16.0 torchaudio==2.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<contextlib.ExitStack at 0x29053ac5590>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import torchvision\n",
    "import time\n",
    "from tempfile import TemporaryDirectory\n",
    "\n",
    "from src.modelvis import visualize_models\n",
    "from src.data_loader import GetDataLoaders, imshow, visualize_models\n",
    "from src.training import eval_func, train_model\n",
    "from src.models import Resnet18_FC_Changed\n",
    "\n",
    "'''\n",
    "The line cudnn.benchmark = True is typically used in deep learning projects that utilize the CUDA Deep Neural Network (cuDNN) library.\n",
    "When cudnn.benchmark is set to True, it enables cuDNN to automatically find the best algorithm configuration for the specific \n",
    "input sizes and hardware being used. This can result in improved performance during training and inference.By enabling benchmarking, \n",
    "cuDNN will run a short benchmarking phase during the first iteration of the model to determine the optimal algorithm configuration. \n",
    "This configuration is then cached and used for subsequent iterations, leading to faster execution times.\n",
    "It's important to note that enabling benchmarking may introduce some overhead during the initial benchmarking phase, so it is typically \n",
    "recommended to use it when the input sizes are consistent throughout the training process.\n",
    "Overall, setting cudnn.benchmark to True can help optimize the performance of deep learning models that use cuDNN.\n",
    "'''\n",
    "cudnn.benchmark = True\n",
    "\n",
    "'''\n",
    "The line plt.ion() is a function call that activates interactive mode in matplotlib.\n",
    "When interactive mode is enabled, any plot that is created will be displayed immediately \n",
    "and can be updated dynamically. This means that you can modify the plot after it is displayed, \n",
    "such as changing the data or adding annotations, and the changes will be reflected in real-time.\n",
    "'''\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16.1+cpu\n",
      "2.1.1+cpu\n"
     ]
    }
   ],
   "source": [
    "print(torchvision.__version__)\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_0 = 1000 # Number of iterations for the first restart.\n",
    "LEARNING_RATE=0.001 # 0.0001\n",
    "WEIGHT_DECAY=0.0005 # 0.000001\n",
    "NUM_EPOCHS=15\n",
    "BATCH_SIZE=64\n",
    "MOMENTUM=0.9\n",
    "num_of_classes = 64\n",
    "STEP_SIZE=7\n",
    "GAMMA=0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHUFFLE=True\n",
    "WORKERS=4\n",
    "num_images = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "data_dir = '..\\\\data\\\\miniImageNet\\\\train'\n",
    "dataloaders, class_names, dataset_sizes  = GetDataLoaders(data_dir, BATCH_SIZE, SHUFFLE, num_workers=WORKERS)\n",
    "\n",
    "train_loader = dataloaders['train']\n",
    "validation_loader = dataloaders['val']\n",
    "test_loader = dataloaders['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please wait patiently, it may take some seconds...\n",
      "No Cuda available\n",
      "Epoch 0/14\n",
      "----------\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "only batches of spatial targets supported (3D tensors) but got targets of dimension: 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[78], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m StepLR(optimizer, step_size\u001b[38;5;241m=\u001b[39mSTEP_SIZE, gamma\u001b[38;5;241m=\u001b[39mGAMMA)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPlease wait patiently, it may take some seconds...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m best_model \u001b[38;5;241m=\u001b[39m train_model(net, dataloaders, criterion, optimizer, scheduler, NUM_EPOCHS, dataset_sizes)\n\u001b[0;32m      9\u001b[0m save_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mmodels\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mbest_model_Resnet18.pth\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     10\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(best_model\u001b[38;5;241m.\u001b[39mstate_dict(), save_path)\n",
      "Cell \u001b[1;32mIn[72], line 71\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, dataloaders, criterion, optimizer, scheduler, num_epochs, dataset_sizes)\u001b[0m\n\u001b[0;32m     69\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(images)\n\u001b[0;32m     70\u001b[0m _, preds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 71\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     72\u001b[0m \u001b[38;5;66;03m# backward + optimize only if in training phase\u001b[39;00m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m phase \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\loss.py:1179\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m-> 1179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mcross_entropy(\u001b[38;5;28minput\u001b[39m, target, weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight,\n\u001b[0;32m   1180\u001b[0m                            ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mignore_index, reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduction,\n\u001b[0;32m   1181\u001b[0m                            label_smoothing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_smoothing)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\functional.py:3053\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3051\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3052\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3053\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mcross_entropy_loss(\u001b[38;5;28minput\u001b[39m, target, weight, _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: only batches of spatial targets supported (3D tensors) but got targets of dimension: 1"
     ]
    }
   ],
   "source": [
    "net = Resnet18_FC_Changed(num_of_classes).to(device)\n",
    "for p in net.parameters():\n",
    "    p.requires_grad = True\n",
    "net.train()\n",
    "optimizer = torch.optim.SGD(params= net.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)\n",
    "print('Please wait patiently, it may take some seconds...')\n",
    "best_model = train_model(net, dataloaders, criterion, optimizer, scheduler, NUM_EPOCHS, dataset_sizes)\n",
    "save_path = '..\\\\data\\\\models\\\\best_model_Resnet18.pth'\n",
    "torch.save(best_model.state_dict(), save_path)\n",
    "\n",
    "eval_acc = eval_func(best_model, dataloaders['test'])\n",
    "print('')\n",
    "print('Accuracy on testing data: %f' % eval_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_model.load_state_dict(torch.load(save_path))\n",
    "visualize_models(best_model, dataloaders, num_images, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
